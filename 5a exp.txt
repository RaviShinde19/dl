üß† THEORY ‚Äî Concepts Behind the Code
üåç 1. What Are Word Embeddings?


Word embeddings are dense vector representations of words, where words with similar meanings have similar numeric representations.


They capture semantic and syntactic relationships (e.g., ‚Äúking - man + woman ‚âà queen‚Äù).


Example:
king ‚Üí [0.21, 0.89, -0.45, ‚Ä¶]
queen ‚Üí [0.20, 0.90, -0.47, ‚Ä¶]

Exam phrase:

‚ÄúWord embeddings map words into continuous vector space so that similar words have nearby vector positions.‚Äù


‚öôÔ∏è 2. What Is CBOW (Continuous Bag of Words)?


CBOW is one of the two architectures from Word2Vec (other is Skip-Gram).


CBOW predicts the target word given surrounding context words.
(Skip-Gram does the reverse.)


Example:

Sentence: ‚Äúdeep learning uses neural networks‚Äù
Context window = 2
Input: [‚Äúdeep‚Äù, ‚Äúuses‚Äù, ‚Äúneural‚Äù, ‚Äúnetworks‚Äù] ‚Üí Predict: ‚Äúlearning‚Äù

So your model tries to learn the relationship between context words and the target word.

üß© 3. Components of the Model
ComponentPurposeTokenizerConverts text to numeric IDs for each unique word.Embedding LayerConverts word IDs to dense continuous vectors.Context WindowDefines how many words before and after are used to predict target.Lambda LayerAverages context word embeddings (CBOW bag-of-words).Dense LayersNon-linear transformations that learn deeper relationships.Softmax LayerOutputs probability for each word as the predicted center word.Loss Function (sparse_categorical_crossentropy)Compares predicted vs actual word IDs.Optimizer (Adam)Updates weights to reduce prediction error.

üìä 4. Learning Objective (simplified math)
Model learns embeddings E so that
P(wt‚à£context)=softmax(W‚ãÖmean(E[context]))P(w_t | context) = softmax(W \cdot mean(E[context]))P(wt‚Äã‚à£context)=softmax(W‚ãÖmean(E[context]))
where wtw_twt‚Äã is target word, E[context]E[context]E[context] are embedding vectors of context words.
Loss = ‚àí log probability of correct target word.
Model minimizes this over all (context, target) pairs.

üß™ 5. What You‚Äôre Building Here
‚úÖ Data cleaning and tokenization
‚úÖ Context-target pair generation (CBOW style)
‚úÖ Neural network (Embedding ‚Üí Mean ‚Üí Dense ‚Üí Softmax)
‚úÖ Train to predict target words
‚úÖ Check learned predictions qualitatively

üîç LINE-BY-LINE CODE EXPLANATION

Text Cleaning & Preprocessing
import numpy as np
import re
from tensorflow.keras.preprocessing.text import Tokenizer



NumPy: handles numeric arrays.


re: regex for cleaning text.


Tokenizer: converts text into numerical word indices.



data = """Deep learning (also known as deep structured learning) ..."""
data = data.split('.')



Multi-sentence text paragraph.


Split by "." ‚Üí gives a list of sentences.



clean_sent=[]
for sentence in data:
    if sentence=="":
        continue
    sentence = re.sub('[^A-Za-z0-9]+', ' ', (sentence))
    sentence = re.sub(r'(?:^| )\w (?:$| )', ' ', (sentence)).strip()
    sentence = sentence.lower()
    clean_sent.append(sentence)

Line-by-line:


if sentence=="": skip empty lines.


re.sub('[^A-Za-z0-9]+', ' ', sentence) ‚Üí removes punctuation/special chars, keeps only letters/numbers.


re.sub(r'(?:^| )\w (?:$| )', ' ', sentence) ‚Üí removes single-character words (like ‚Äúa‚Äù, ‚ÄúI‚Äù).


.strip() ‚Üí removes leading/trailing spaces.


.lower() ‚Üí convert to lowercase for uniformity.


Append to clean_sent.


‚úÖ Now clean_sent is a list of clean, lowercase sentences ‚Äî suitable for tokenization.

Tokenization and Vocabulary Building
tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_sent)
word2idx = tokenizer.word_index
idx2word = {v: k for k, v in word2idx.items()}
vocab_size = len(word2idx) + 1



Tokenizer() ‚Üí builds mapping of word ‚Üí index.


fit_on_texts ‚Üí reads all words and assigns unique numeric IDs (starting from 1).


word2idx ‚Üí dictionary of word‚Üíid.


idx2word ‚Üí reverse dictionary (id‚Üíword) for decoding.


vocab_size ‚Üí total number of unique words + 1 (because indexing starts from 1).


‚úÖ Vocabulary created.

sequences = tokenizer.texts_to_sequences(clean_sent)



Converts each sentence into a list of numeric IDs (based on the tokenizer mapping).
Example:
‚Äúdeep learning methods‚Äù ‚Üí [1, 2, 3].



Creating Context-Target Pairs (CBOW)
emb_size = 10
context_size = 2

contexts = []
targets = []

for sequence in sequences:
    for i in range(context_size, len(sequence) - context_size):
        target = sequence[i]
        context = [sequence[i - 2], sequence[i - 1], sequence[i + 1], sequence[i + 2]]
        contexts.append(context)
        targets.append(target)

Explanation:


context_size = 2 ‚Üí Use 2 words before and 2 words after as context.


Loop through each word position i in a sentence:


target = sequence[i] ‚Üí current word (center).


context = [sequence[i-2], i-1, i+1, i+2] ‚Üí 4 context words.


Store them.




‚úÖ After this, we have:


contexts: list of context-word indices.


targets: list of target-word indices.


Example:
Sentence: ‚Äúdeep learning uses neural networks‚Äù
Context (deep, uses, neural, networks) ‚Üí Target: ‚Äúlearning‚Äù

for i in range(5):
    words = []
    target = idx2word.get(targets[i])
    for j in contexts[i]:
        words.append(idx2word.get(j))
    print(words," -> ", target)



Decodes the first 5 (context ‚Üí target) pairs using idx2word.


Helps visually verify training data correctness.



X = np.array(contexts)
Y = np.array(targets)



Convert lists to NumPy arrays for training.


X = inputs (shape: samples √ó 4)


Y = outputs (shape: samples, )





Model Architecture ‚Äî CBOW using Keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Lambda
from tensorflow.keras import backend as K



Sequential: simple stack model.


Dense: fully connected layer.


Embedding: converts integer word IDs to dense vectors.


Lambda: lets you define custom layer operations (here, averaging embeddings).


K: backend operations if needed (TensorFlow math functions).



üß† Building the CBOW Model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size),
    Lambda(lambda x: tf.reduce_mean(x, axis=1)),
    Dense(256, activation='relu'),
    Dense(512, activation='relu'),
    Dense(vocab_size, activation='softmax')
])

Line-by-line:


Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size)


Converts input word indices ‚Üí embedding vectors.


input_dim: number of unique words.


output_dim: embedding size (10 here).


input_length: context length (4 words total for 2 before + 2 after).


Why Embedding? Learns dense vector representations (word embeddings) during training.


Exam phrase: ‚ÄúEmbedding layer transforms sparse word IDs into learnable dense vectors.‚Äù





Lambda(lambda x: tf.reduce_mean(x, axis=1))


Takes the mean of all context embeddings ‚Üí single averaged vector (CBOW assumption: bag of words).


Exam: ‚ÄúLambda layer averages the embeddings of context words.‚Äù







Dense(256, activation='relu')


Dense(512, activation='relu')


Two hidden fully-connected layers to learn complex non-linear patterns.


ReLU: Rectified Linear Unit ‚Üí f(x)=max(0,x)f(x)=max(0,x)f(x)=max(0,x)


Exam: ‚ÄúDense layers capture relationships between averaged context and target.‚Äù







Dense(vocab_size, activation='softmax')


Output layer ‚Üí predicts probability of every word in the vocabulary being the target word.


Softmax: ensures probabilities sum to 1.


Exam: ‚ÄúSoftmax converts network output into a probability distribution over vocabulary.‚Äù





model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])



Loss: Sparse categorical crossentropy (for integer class labels).


Optimizer: Adam (adaptive learning rate).


Metric: Accuracy for training feedback.


Exam phrase:

‚ÄúWe compile with Adam optimizer and crossentropy loss because it‚Äôs a multi-class classification problem.‚Äù


model.fit(X, Y, epochs=200, verbose=0)



Train model for 200 epochs silently (verbose=0).


Learns embeddings + dense layer weights to minimize loss.



Visualization
import seaborn as sns
sns.lineplot(model.history.history)



Plots the history dictionary (loss, accuracy vs epoch).
(Note: should use pd.DataFrame(model.history.history) for proper plotting.)



Prediction on Test Sentences
test_sentenses = [
    "known as structured learning",
    "transformers have applied to",
    "where they produced results",
    "cases surpassing expert performance"
]



List of small test contexts to check model predictions.



for sent in test_sentenses:
    test_words = sent.split(" ")
    x_test = []
    for i in test_words:
        x_test.append(word_to_index.get(i))
    x_test = np.array([x_test])

    pred = model.predict(x_test)
    pred = np.argmax(pred[0])
    print("pred ", test_words, "\n=", index_to_word.get(pred),"\n\n")

Explanation:


For each test phrase:


Convert words ‚Üí indices.


Predict using trained model.


np.argmax(pred[0]) ‚Üí ID of word with highest predicted probability.


index_to_word.get(pred) ‚Üí Convert back to actual word.




‚úÖ Model predicts the most likely central word for that context.

üß© Summary Table (Viva-ready)
ConceptDescriptionKey KeywordWord EmbeddingDense representation of words in vector spaceSemantic similarityCBOW ModelPredicts center word from contextWord2Vec architectureTokenizerConverts words ‚Üí indicesPreprocessingEmbedding LayerLearns dense word vectorsRepresentation learningLambda LayerAverages embeddingsCBOW ‚Äúbag of words‚ÄùDense LayerFully connected neural layerFeature learningSoftmaxConverts logits ‚Üí probabilitiesMulti-class outputLossSparse categorical crossentropyClassification lossOptimizerAdam (adaptive SGD)Faster convergenceContext WindowWords before & after targetCBOW windowReconstruction GoalContext ‚Üí Target wordCore idea

‚úÖ Short viva line you can say:

‚ÄúI implemented a CBOW-based word embedding model. It uses an Embedding layer to represent words, averages context embeddings using a Lambda layer, passes them through Dense layers, and uses a softmax output to predict the central word. The model learns semantic relationships between words automatically.‚Äù


Would you like me to next:


Add inline comments to your code (every line explained right inside it for submission),
or


Give a short viva script (under 1 minute) summarizing how to explain this entire practical orally?

